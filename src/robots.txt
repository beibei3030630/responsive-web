搜索引擎用来访问网站要查看的第一个文件。robots告诉搜索的蜘蛛程序在服务器上什么文件可以被查看，什么文件不能被查看，当搜索爬虫访问到一个网站的时候，首先会查看该网站根目录下面是否存在这样一个robots.txt文件。如果存在搜索机器人会根据这个文件的一些内容来确定查看的范围，如果不存在，搜索爬虫会访问网站上所有没有被口令保护的页面。一般性，网站总有管理页面和后台页面不希望搜索引擎抓取。所以我们可以通过robots.txt这个文件来告诉搜索引擎不要抓取哪些文件。

例子（基本格式）：User-agent:*
Disallow:/admin/
告诉搜索引擎都可以去访问，除了admin文件下的内容。
